{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy as cp\n",
    "import importanceMatrix\n",
    "import shap\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.preprocessing import scale\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "proj_path = os.path.dirname(os.getcwd())\n",
    "fig_path = proj_path + '/2_docs/LaTeX/Figures'\n",
    "datasets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title='Normalized confusion matrix',\n",
    "                          cmap=plt.cm.plasma):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    #print(cm)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax, label='Percentage')\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.0f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(round(cm[i, j],0), fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"black\" if cm[i, j] > thresh else \"white\")\n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_features(rf_clf, feature_names, X_train, y_train, y):\n",
    "    #Global importance ranking\n",
    "    global_importance = rf_clf.feature_importances_\n",
    "    global_index = np.argsort(global_importance)\n",
    "    global_index = global_index[::-1]\n",
    "    \n",
    "    #Shap values ranking\n",
    "    explainer = shap.TreeExplainer(rf_clf)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    shap_imp_mat = np.array([np.mean(abs(class_shap), axis=0) for class_shap in shap_values])\n",
    "    ###NORMALIZE ROWS\n",
    "    row_sums = shap_imp_mat.sum(axis=1)\n",
    "    shap_imp_mat = shap_imp_mat / row_sums[:, np.newaxis]\n",
    "    ###\n",
    "    shap_imp = np.mean(shap_imp_mat, axis=0)\n",
    "    shap_index = np.argsort(shap_imp)\n",
    "    shap_index = shap_index[::-1]\n",
    "\n",
    "    #Per-class importance ranking\n",
    "    imp_mat = importanceMatrix.calcImportanceMatrix(rf_clf)\n",
    "    mean_importance = np.mean(imp_mat, axis=0)\n",
    "    pcfi_index = np.argsort(mean_importance)\n",
    "    pcfi_index = pcfi_index[::-1]\n",
    "\n",
    "    ###check y_train rare class\n",
    "    unique_classes, count_classes = np.unique(y_train, return_counts=True)\n",
    "    rare_class_index = np.argsort(count_classes)[0]\n",
    "    rare_class_train = unique_classes[rare_class_index]\n",
    "    \n",
    "    unique_classes, count_classes = np.unique(y, return_counts=True)\n",
    "    rare_class_index = np.argsort(count_classes)[0]\n",
    "    rare_class = unique_classes[rare_class_index]\n",
    "    print('Do y and y_train have same rare class?', rare_class==rare_class_train)\n",
    "    \n",
    "    rare_class_pcfi_index = np.argsort(imp_mat[rare_class_index])\n",
    "    rare_class_pcfi_index = rare_class_pcfi_index[::-1]\n",
    "    \n",
    "    rare_class_shap_index = np.argsort(shap_imp_mat[rare_class_index])\n",
    "    rare_class_shap_index = rare_class_shap_index[::-1]\n",
    "    \n",
    "    ##STOP AT THE FIRST DIFFERING INSTANCE\n",
    "    index = [i for i in np.arange(len(feature_names)) if shap_index[i] != pcfi_index[i]]#SHAP OR GLOBAL?\n",
    "    print(index)\n",
    "    if len(index) > 0 and index[0] > 2:\n",
    "        top_global_features = feature_names[global_index[:index[0]+1]]\n",
    "        top_shap_features = feature_names[shap_index[:index[0]+1]]\n",
    "        top_pcfi_features = feature_names[pcfi_index[:index[0]+1]]\n",
    "        #top_rare_pcfi_features = feature_names[rare_class_pcfi_index[:index[0]+1]]\n",
    "        #top_rare_shap_features = feature_names[rare_class_shap_index[:index[0]+1]]\n",
    "        #return [top_global_features, top_shap_features, top_pcfi_features, \n",
    "        #        top_rare_shap_features, top_rare_pcfi_features], rare_class\n",
    "    else:\n",
    "        top_global_features = feature_names[global_index[:3]]\n",
    "        top_shap_features = feature_names[shap_index[:3]]\n",
    "        top_pcfi_features = feature_names[pcfi_index[:3]]\n",
    "    top_rare_pcfi_features = feature_names[rare_class_pcfi_index[:3]]\n",
    "    top_rare_shap_features = feature_names[rare_class_shap_index[:3]]\n",
    "    \n",
    "    if all(lab in top_rare_pcfi_features for lab in top_rare_shap_features):\n",
    "        top_rare_shap_features = top_rare_pcfi_features\n",
    "    #REORDER EQUAL LABS\n",
    "    top_features_lst = [top_global_features, top_shap_features, top_pcfi_features, \n",
    "                        top_rare_shap_features, top_rare_pcfi_features]\n",
    "    for k in np.arange(2):\n",
    "        f1 = top_features_lst[k]\n",
    "        for j in np.arange(k+1,3):\n",
    "            f2 = top_features_lst[j]\n",
    "            if all(lab in f1 for lab in f2):\n",
    "                print(k,j)\n",
    "                top_features_lst[j] = f1\n",
    "    return top_features_lst, rare_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_scores(y_test, predictions):\n",
    "    #errors = abs(predictions - y_test)\n",
    "    acc = 100 * accuracy_score(y_test, predictions)\n",
    "    f1score = 100 * f1_score(y_test, predictions, average='macro')\n",
    "    return (round(acc, 2), round(f1score, 2))#round(np.mean(errors), 2), \n",
    "\n",
    "def fitRF_and_rank(data, feature_names, class_col, rnd_seed=45):#class_names, class_tags, \n",
    "    X = np.array(data.loc[:,feature_names])\n",
    "    y = np.array(data.loc[:,class_col])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rnd_seed)\n",
    "    \n",
    "    #Print dataset info and check that all classes are represented in  y_train and y_test \n",
    "    unique_classes, count_classes = np.unique(y, return_counts=True)\n",
    "    is_balanced = all([i==j for i in count_classes for j in count_classes])\n",
    "    print('Is the dataset balanced?', is_balanced)\n",
    "    print(unique_classes, count_classes)\n",
    "    print(np.unique(y_train, return_counts=True))\n",
    "    print(np.unique(y_test, return_counts=True))\n",
    "    \n",
    "    # Train the classifier\n",
    "    rf_gscv = GridSearchCV(estimator=RandomForestClassifier(random_state=rnd_seed),\n",
    "                          param_grid={'n_estimators':[10, 25, 50, 75, 100, 150, 200, 250, 300],\n",
    "                                      'max_depth': [10, 15, 20, 25, None]},\n",
    "                          scoring='f1_macro', cv=3, iid=False)\n",
    "    rf_gscv.fit(X_train, y_train)\n",
    "    print('Best score:', rf_gscv.best_score_)\n",
    "    rf_clf = rf_gscv.best_estimator_\n",
    "          \n",
    "    predictions = rf_clf.predict(X_test)\n",
    "    accuracy, f1score = fit_scores(y_test, predictions)\n",
    "    print('Model performances:')\n",
    "    print('Accuracy: {}'.format(accuracy))\n",
    "    print('F1 macro score : {}'.format(f1score))\n",
    "    top_feature_lst, rare_class = top_features(rf_clf, feature_names, X_train, y_train, y)\n",
    "    return (rf_gscv, top_feature_lst, rare_class)\n",
    "\n",
    "def refitRF(data, feature_names, class_col, rf_gscv, rare_class, rnd_seed=45, global_score=False):\n",
    "    X = np.array(data.loc[:,feature_names])\n",
    "    y = np.array(data.loc[:,class_col])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rnd_seed)\n",
    "    \n",
    "    # Train the classifier\n",
    "    rf_clf = RandomForestClassifier(random_state=rnd_seed,\n",
    "                                    max_depth=rf_gscv.best_params_['max_depth'],\n",
    "                                    n_estimators=rf_gscv.best_params_['n_estimators']\n",
    "                                   )\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "          \n",
    "    predictions = rf_clf.predict(X_test)\n",
    "    if global_score:\n",
    "        score = fit_scores(y_test, predictions)\n",
    "        #print('Model performances:')\n",
    "        #print('Accuracy: {}'.format(accuracy))\n",
    "        #print('F1 macro score : {}'.format(f1score))\n",
    "    else:\n",
    "        print(rare_class)\n",
    "        score = f1_score(y_test, predictions, labels=[rare_class], average=None)[0]\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the dataset balanced? False\n",
      "[1 2 3 4 5 6] [111  60  71  48  48  20]\n",
      "(array([1, 2, 3, 4, 5, 6]), array([80, 46, 58, 34, 36, 14]))\n",
      "(array([1, 2, 3, 4, 5, 6]), array([31, 14, 13, 14, 12,  6]))\n",
      "Best score: 0.9827938369873853\n",
      "Model performances:\n",
      "Accuracy: 98.89\n",
      "F1 macro score : 98.81\n",
      "Do y and y_train have same rare class? True\n",
      "[0, 1, 2, 3, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32]\n",
      "Global:  ['Clubbing of the rete ridges' 'Fibrosis of the papillary dermis'\n",
      " 'Thinning of the suprapapillary epidermis']\n",
      "Shap:  ['Clubbing of the rete ridges' 'Fibrosis of the papillary dermis'\n",
      " 'Elongation of the rete ridges']\n",
      "Pcfi:  ['Fibrosis of the papillary dermis' 'Clubbing of the rete ridges'\n",
      " 'Koebner phenomenon']\n",
      "Rare class Shap:  ['Perifollicular parakeratosis' 'Knee and elbow involvement'\n",
      " 'Follicular horn plug']\n",
      "Rare class Pcfi:  ['Perifollicular parakeratosis' 'Follicular horn plug'\n",
      " 'Follicular papules']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "Global (63.33, 45.44)\n",
      "Shap (63.33, 43.87)\n",
      "Pcfi (76.67, 62.59)\n",
      "Rare Shap 1.0\n",
      "Rare Pfci 1.0\n"
     ]
    }
   ],
   "source": [
    "#Import dermatology data #PCFI ONLY GLOBAL\n",
    "col_names = np.array([\n",
    "    'erythema', 'scaling', 'definite borders',\n",
    "    'itching', 'koebner phenomenon', 'polygonal papules',\n",
    "    'follicular papules', 'oral mucosal involvement', 'knee and elbow involvement',\n",
    "    'scalp involvement', 'family history', 'melanin incontinence',\n",
    "    'eosinophils in the infiltrate', 'PNL infiltrate', 'fibrosis of the papillary dermis',\n",
    "    'exocytosis', 'acanthosis', 'hyperkeratosis',\n",
    "    'parakeratosis', 'clubbing of the rete ridges', 'elongation of the rete ridges',\n",
    "    'thinning of the suprapapillary epidermis', 'spongiform pustule', 'munro microabcess',\n",
    "    'focal hypergranulosis', 'disappearance of the granular layer',\n",
    "    'vacuolisation and damage of basal layer',\n",
    "    'spongiosis', 'saw-tooth appearance of retes', 'follicular horn plug',\n",
    "    'perifollicular parakeratosis', 'inflammatory monoluclear inflitrate',\n",
    "    'band-like infiltrate',\n",
    "    'Age', 'Class'\n",
    "])\n",
    "col_names = np.array([lab.capitalize() for lab in col_names])\n",
    "feature_names = np.array(col_names[:-1])\n",
    "class_col = col_names[-1]\n",
    "class_names = np.array(['psoriasis', 'seboreic dermatitis', 'lichen planus',\n",
    "                        'pityriasis rosea', 'cronic dermatitis', 'pityriasis rubra pilaris'])\n",
    "class_names = np.array([lab.capitalize() for lab in class_names])\n",
    "class_tags = np.arange(len(class_names)) + 1\n",
    "data = pd.read_csv(proj_path+'/0_data/dermatology.data.csv', header=None, names=col_names)\n",
    "skip_rows = data.Age == '?'\n",
    "data = data[~skip_rows]\n",
    "data.Age = np.array(data.Age, dtype=int)\n",
    "\n",
    "rnd_seed = 45\n",
    "data_info = ['Dermatology', data, feature_names, class_col, class_names, class_tags, rnd_seed]\n",
    "datasets.append(data_info)\n",
    "\n",
    "###\n",
    "rf_gscv, top_features_lst, rare_class = fitRF_and_rank(data, feature_names, class_col, rnd_seed)\n",
    "for lab,feat in zip(['Global: ', 'Shap: ', 'Pcfi: ', 'Rare class Shap: ', 'Rare class Pcfi: '], top_features_lst):\n",
    "    print(lab,feat)\n",
    "label_lst = ['Global', 'Shap', 'Pcfi', 'Rare Shap', 'Rare Pfci']\n",
    "is_global_score = [True, True, True, False, False]\n",
    "scores = [refitRF(data, top_features, class_col, rf_gscv, rare_class, rnd_seed, global_score=global_score)\n",
    "             for global_score,top_features in zip(is_global_score,top_features_lst)]\n",
    "for lab,score in zip(label_lst, scores):\n",
    "    print(lab, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the dataset balanced? False\n",
      "[1 2 3 4] [27 54 42  8]\n",
      "(array([1, 2, 3, 4]), array([22, 41, 29,  6]))\n",
      "(array([1, 2, 3, 4]), array([ 5, 13, 13,  2]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.5892526488466422\n",
      "Model performances:\n",
      "Accuracy: 69.7\n",
      "F1 macro score : 63.28\n",
      "Do y and y_train have same rare class? True\n",
      "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "Global:  ['Class x percentage' 'Internal assessment percentage'\n",
      " 'Class xii percentage' 'Father qualification']\n",
      "Shap:  ['Class x percentage' 'Internal assessment percentage'\n",
      " 'Class xii percentage' 'Home to college travel time']\n",
      "Pcfi:  ['Class x percentage' 'Internal assessment percentage'\n",
      " 'Class xii percentage' 'Mother qualification']\n",
      "Rare class Shap:  ['Internal assessment percentage' 'Class xii percentage'\n",
      " 'Class x percentage']\n",
      "Rare class Pcfi:  ['Internal assessment percentage' 'Class xii percentage'\n",
      " 'Class x percentage']\n",
      "4\n",
      "4\n",
      "Global (54.55, 40.46)\n",
      "Shap (51.52, 35.53)\n",
      "Pcfi (66.67, 53.97)\n",
      "Rare Shap 0.0\n",
      "Rare Pfci 0.0\n"
     ]
    }
   ],
   "source": [
    "#Students data #PCFI ONLY GLOBAL\n",
    "col_names = np.array([\n",
    "    'Gender', 'Caste', 'Class X Percentage', 'Class XII Percentage', 'Internal Assessment Percentage',\n",
    "    'End Semester Percentage', 'Whether the student has back or arrear papers', 'Marital Status',\n",
    "    'Lived in Town or Village', 'Admission Category', 'Family Monthly Income', 'Family Size',\n",
    "    'Father Qualification', 'Mother Qualification', 'Father Occupation', 'Mother Occupation',\n",
    "    'Number of Friends', 'Study Hours', 'Student School attended at Class X level', 'Medium',\n",
    "    'Home to College Travel Time', 'Class Attendance Percentage'\n",
    "])\n",
    "col_names = np.array([lab.capitalize() for lab in col_names])\n",
    "students_data = pd.read_csv(proj_path+'/0_data/Student_performances.csv', header=None, names=col_names)\n",
    "students_data = students_data.loc[:,col_names!='Marital status']\n",
    "\n",
    "string_to_int_list = [\n",
    "    {'M':0,'F':1}, {'G':0,'ST':1,'SC':2,'OBC':3,'MOBC':4}, {'Best':4,'Vg':3,'Good':2,'Pass':1,'Fail':0},\n",
    "    {'Best':4,'Vg':3,'Good':2,'Pass':1,'Fail':0}, {'Best':4,'Vg':3,'Good':2,'Pass':1,'Fail':0},\n",
    "    {'Best':4,'Vg':3,'Good':2,'Pass':1,'Fail':0}, {'Y':1,'N':0},\n",
    "    {'T':1,'V':0}, {'Free':0,'Paid':1},\n",
    "    {'Vh':4,'High':3,'Am':2,'Medium':1,'Low':0}, {'Large':2,'Average':1,'Small':0},\n",
    "    {'Il':0,'Um':1,'10':2,'12':3,'Degree':4,'Pg':5}, {'Il':0,'Um':1,'10':2,'12':3,'Degree':4,'Pg':5},\n",
    "    {'Service':0,'Business':1,'Retired':2,'Farmer':3,'Others':4},\n",
    "    {'Service':0,'Business':1,'Retired':2,'Housewife':3,'Others':4},\n",
    "    {'Large':2,'Average':1,'Small':0}, {'Good':2,'Average':1,'Poor':0},\n",
    "    {'Govt':1,'Private':0}, {'Eng':0,'Asm':1,'Hin':2,'Ben':3},\n",
    "    {'Large':2,'Average':1,'Small':0}, {'Good':2,'Average':1,'Poor':0}\n",
    "]\n",
    "for col_name,string_to_int in zip(students_data.columns, string_to_int_list):\n",
    "    #print(col_name,string_to_int,\n",
    "    #     students_data.apply(lambda r: string_to_int[r[col_name]], axis=1))\n",
    "    students_data.loc[:,col_name] = students_data.apply(lambda r: string_to_int[r[col_name]], axis=1)\n",
    "    \n",
    "feature_names = np.array(students_data.columns[students_data.columns!='End semester percentage'])\n",
    "class_col = 'End semester percentage'\n",
    "class_names = np.array(['Fail', 'Pass', 'Good', 'Vg', 'Best'])\n",
    "class_tags = np.arange(len(class_names))\n",
    "#print(feature_names, class_names, class_tags)\n",
    "\n",
    "#44 70% accuracy, makes no point; 45 no rare class to test;46 too low acc; 47\n",
    "rnd_seed = 44\n",
    "\n",
    "data_info = ['Student_finals', students_data, feature_names, class_col, class_names, class_tags, rnd_seed]\n",
    "datasets.append(data_info)\n",
    "\n",
    "\n",
    "###\n",
    "data = students_data\n",
    "rf_gscv, top_features_lst, rare_class = fitRF_and_rank(data, feature_names, class_col, rnd_seed)\n",
    "for lab,feat in zip(['Global: ', 'Shap: ', 'Pcfi: ', 'Rare class Shap: ', 'Rare class Pcfi: '], top_features_lst):\n",
    "    print(lab,feat)\n",
    "label_lst = ['Global', 'Shap', 'Pcfi', 'Rare Shap', 'Rare Pfci']\n",
    "is_global_score = [True, True, True, False, False]\n",
    "scores = [refitRF(data, top_features, class_col, rf_gscv, rare_class, rnd_seed, global_score=global_score)\n",
    "             for global_score,top_features in zip(is_global_score,top_features_lst)]\n",
    "for lab,score in zip(label_lst, scores):\n",
    "    print(lab, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the dataset balanced? False\n",
      "[ 1  2  5 11 12 14 18 22] [82 20 39 28 16 24 29 24]\n",
      "(array([ 1,  2,  5, 11, 12, 14, 18, 22]), array([59, 15, 32, 23,  9, 21, 25, 12]))\n",
      "(array([ 1,  2,  5, 11, 12, 14, 18, 22]), array([23,  5,  7,  5,  7,  3,  4, 12]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.46806892045862636\n",
      "Model performances:\n",
      "Accuracy: 53.03\n",
      "F1 macro score : 46.63\n",
      "Do y and y_train have same rare class? True\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Global:  ['Sex' 'Age' 'Lung']\n",
      "Shap:  ['Peritoneum' 'Sex' 'Liver']\n",
      "Pcfi:  ['Sex' 'Age' 'Abdominal']\n",
      "Rare class Shap:  ['Age' 'Sex' 'Abdominal']\n",
      "Rare class Pcfi:  ['Age' 'Sex' 'Abdominal']\n",
      "12\n",
      "12\n",
      "Global (25.76, 14.21)\n",
      "Shap (39.39, 17.28)\n",
      "Pcfi (34.85, 20.06)\n",
      "Rare Shap 0.588235294117647\n",
      "Rare Pfci 0.588235294117647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Import tumor data #PCFI GLOBAL\n",
    "col_names = np.array([\n",
    "    'class', 'age', 'sex', 'histologic-type', 'degree-of-diffe', 'bone', 'bone-marrow', 'lung', 'pleura',\n",
    "    'peritoneum', 'liver', 'brain', 'skin', 'neck', 'supraclavicular', 'axillar', 'mediastinum', 'abdominal'\n",
    "])\n",
    "col_names = np.array([lab.capitalize() for lab in col_names])\n",
    "feature_names = col_names[col_names!='Class']\n",
    "class_names = np.array([\n",
    "    'lung', 'head & neck', 'esophasus', 'thyroid', 'stomach', 'duoden & sm.int',\n",
    "    'colon', 'rectum', 'anus', 'salivary glands', 'pancreas', 'gallblader',\n",
    "    'liver', 'kidney', 'bladder', 'testis', 'prostate', 'ovary', 'corpus uteri', \n",
    "    'cervix uteri', 'vagina', 'breast'\n",
    "])\n",
    "#class_tags = np.arange(len(class_names))\n",
    "tumor_data = pd.read_csv(proj_path+'/0_data/primary-tumor.data.csv', header=None, names=col_names)\n",
    "tumor_data = tumor_data.loc[:,[col for col in col_names if not(col in ['Histologic-type', 'Degree-of-diffe'])]]\n",
    "tumor_data.apply(lambda r: any(r=='?'),axis=1)\n",
    "tumor_data = tumor_data.loc[~(tumor_data.apply(lambda r: any(r=='?'),axis=1)),:]\n",
    "#print(np.unique(tumor_data.Class, return_counts=True))\n",
    "class_tags, class_counts = np.unique(tumor_data.Class, return_counts=True)\n",
    "keep = class_tags[class_counts>=15]\n",
    "\n",
    "#low_data_classes = np.array([6, 9, 10, 15, 16, 20, 21])\n",
    "\n",
    "#print('Keep classes', class_names[keep-1])\n",
    "tumor_data = tumor_data.loc[tumor_data.Class.isin(keep)]\n",
    "\n",
    "col_names = tumor_data.columns\n",
    "feature_names = np.array(col_names[col_names!='Class'])\n",
    "class_names = np.array([col for col in class_names if col in class_names[keep-1]])\n",
    "class_tags = np.unique(tumor_data.Class)\n",
    "class_col = 'Class'\n",
    "#print(class_names, feature_names, np.unique(tumor_data.Class, return_counts=True))\n",
    "#45\n",
    "rnd_seed = 45\n",
    "data_info = ['Tumor', tumor_data, feature_names, class_col, class_names, class_tags, rnd_seed]\n",
    "datasets.append(data_info)\n",
    "\n",
    "###\n",
    "data = tumor_data\n",
    "rf_gscv, top_features_lst, rare_class = fitRF_and_rank(data, feature_names, class_col, rnd_seed)\n",
    "for lab,feat in zip(['Global: ', 'Shap: ', 'Pcfi: ', 'Rare class Shap: ', 'Rare class Pcfi: '], top_features_lst):\n",
    "    print(lab,feat)\n",
    "label_lst = ['Global', 'Shap', 'Pcfi', 'Rare Shap', 'Rare Pfci']\n",
    "is_global_score = [True, True, True, False, False]\n",
    "scores = [refitRF(data, top_features, class_col, rf_gscv, rare_class, rnd_seed, global_score=global_score)\n",
    "             for global_score,top_features in zip(is_global_score,top_features_lst)]\n",
    "for lab,score in zip(label_lst, scores):\n",
    "    print(lab, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the dataset balanced? False\n",
      "[0 1 2 3 4 5] [40 60 36 27 15 16]\n",
      "(array([0, 1, 2, 3, 4, 5]), array([33, 47, 27, 18,  9, 11]))\n",
      "(array([0, 1, 2, 3, 4, 5]), array([ 7, 13,  9,  9,  6,  5]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.4388230236841348\n",
      "Model performances:\n",
      "Accuracy: 61.22\n",
      "F1 macro score : 53.81\n",
      "Do y and y_train have same rare class? True\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "0 2\n",
      "Global:  ['Language' 'Landmass' 'Population']\n",
      "Shap:  ['Landmass' 'Language' 'Zone']\n",
      "Pcfi:  ['Language' 'Landmass' 'Population']\n",
      "Rare class Shap:  ['Zone' 'Landmass' 'Language']\n",
      "Rare class Pcfi:  ['Area' 'Sunstars' 'Language']\n",
      "4\n",
      "4\n",
      "Global (61.22, 50.71)\n",
      "Shap (61.22, 59.32)\n",
      "Pcfi (61.22, 50.71)\n",
      "Rare Shap 0.5\n",
      "Rare Pfci 0.0\n"
     ]
    }
   ],
   "source": [
    "#Import flag data #SHAP WINS BOTH\n",
    "col_names = np.array([\n",
    "    'name', 'landmass', 'zone', 'area', 'population', 'language', 'religion', 'bars', 'stripes', 'colours',\n",
    "    'red', 'green', 'blue', 'gold', 'white', 'black', 'orange', 'mainhue', 'circles', 'crosses', 'saltires',\n",
    "    'quarters', 'sunstars', 'crescent', 'triangle', 'icon', 'animate', 'text', 'topleft', 'botright'\n",
    "])\n",
    "col_names = np.array([lab.capitalize() for lab in col_names])\n",
    "\n",
    "feature_names = np.array(col_names[np.logical_and(col_names!='Religion', col_names!='Name')])\n",
    "class_col = 'Religion'\n",
    "class_names = np.array(['Catholic', 'Other Christian', 'Muslim',# 'Buddhist', 'Hindu',\n",
    "                        'Ethnic', 'Marxist', 'Others'])\n",
    "\n",
    "class_tags = np.arange(len(class_names))\n",
    "flag_data = pd.read_csv(proj_path+'/0_data/flag.data.csv', header=None, names=col_names)\n",
    "\n",
    "string_to_int = {'black':0, 'blue':1, 'brown':2, 'gold':3, 'green':4, 'orange':5, 'red':6, 'white':7}\n",
    "for col_name in ['Mainhue', 'Topleft', 'Botright']:\n",
    "    #print(col_name,string_to_int)\n",
    "    flag_data.loc[:,col_name] = flag_data.apply(lambda r: string_to_int[r[col_name]], axis=1)\n",
    "string_to_int = {0:0, 1:1, 2:2, 3:5, 4:5, 5:3, 6:4, 7:5}\n",
    "flag_data.loc[:,['Religion']] = flag_data.apply(lambda r: string_to_int[r['Religion']], axis=1)\n",
    "\n",
    "#45 (lower than 70 accuracy)\n",
    "rnd_seed = 45\n",
    "\n",
    "data_info = ['Flag', flag_data, feature_names, class_col, class_names, class_tags, rnd_seed]\n",
    "datasets.append(data_info)\n",
    "\n",
    "\n",
    "###\n",
    "data = flag_data\n",
    "rf_gscv, top_features_lst, rare_class = fitRF_and_rank(data, feature_names, class_col, rnd_seed)\n",
    "for lab,feat in zip(['Global: ', 'Shap: ', 'Pcfi: ', 'Rare class Shap: ', 'Rare class Pcfi: '], top_features_lst):\n",
    "    print(lab,feat)\n",
    "label_lst = ['Global', 'Shap', 'Pcfi', 'Rare Shap', 'Rare Pfci']\n",
    "is_global_score = [True, True, True, False, False]\n",
    "scores = [refitRF(data, top_features, class_col, rf_gscv, rare_class, rnd_seed, global_score=global_score)\n",
    "             for global_score,top_features in zip(is_global_score,top_features_lst)]\n",
    "for lab,score in zip(label_lst, scores):\n",
    "    print(lab, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class          I0     PA500       HFS          DA          Area       A/DA  \\\n",
      "0      1  524.794072  0.187448  0.032114  228.800228   6843.598481  29.910803   \n",
      "1      1  330.000000  0.226893  0.265290  121.154201   3163.239472  26.109202   \n",
      "2      1  551.879287  0.232478  0.063530  264.804935  11888.391827  44.894903   \n",
      "3      1  380.000000  0.240855  0.286234  137.640111   5402.171180  39.248524   \n",
      "4      1  362.831266  0.200713  0.244346  124.912559   3290.462446  26.342127   \n",
      "\n",
      "      Max IP          DR           P  \n",
      "0  60.204880  220.737212  556.828334  \n",
      "1  69.717361   99.084964  400.225776  \n",
      "2  77.793297  253.785300  656.769449  \n",
      "3  88.758446  105.198568  493.701814  \n",
      "4  69.389389  103.866552  424.796503  \n",
      "['I0' 'PA500' 'HFS' 'DA' 'Area' 'A/DA' 'Max IP' 'DR' 'P']\n",
      "Is the dataset balanced? False\n",
      "[0 1 2 3 4 5] [22 21 14 15 16 18]\n",
      "(array([0, 1, 2, 3, 4, 5]), array([17, 14, 11, 11, 12, 14]))\n",
      "(array([0, 1, 2, 3, 4, 5]), array([5, 7, 3, 4, 4, 4]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6238492988492989\n",
      "Model performances:\n",
      "Accuracy: 74.07\n",
      "F1 macro score : 70.94\n",
      "Do y and y_train have same rare class? True\n",
      "[5, 6]\n",
      "0 1\n",
      "Global:  ['I0' 'P' 'DA' 'PA500' 'Max IP' 'A/DA']\n",
      "Shap:  ['I0' 'P' 'DA' 'PA500' 'Max IP' 'A/DA']\n",
      "Pcfi:  ['I0' 'P' 'DA' 'PA500' 'Max IP' 'DR']\n",
      "Rare class Shap:  ['I0' 'P' 'DA']\n",
      "Rare class Pcfi:  ['I0' 'P' 'DA']\n",
      "2\n",
      "2\n",
      "Global (81.48, 79.89)\n",
      "Shap (81.48, 79.89)\n",
      "Pcfi (77.78, 74.11)\n",
      "Rare Shap 1.0\n",
      "Rare Pfci 1.0\n"
     ]
    }
   ],
   "source": [
    "#Breast tissue data # BAD PCFI GLOBAL\n",
    "\n",
    "breast_data = pd.read_excel(proj_path+'/0_data/BreastTissue.xls', sheet_name=1)\n",
    "breast_data = breast_data.iloc[:,1:]\n",
    "\n",
    "feature_names = np.array(breast_data.columns[1:])\n",
    "class_col = 'Class'\n",
    "class_names = np.unique(breast_data.Class)\n",
    "class_tags = np.arange(len(class_names))\n",
    "\n",
    "string_to_int = dict(zip(class_names, class_tags))\n",
    "breast_data.loc[:,'Class'] = breast_data.apply(lambda r: string_to_int[r.Class], axis=1)\n",
    "\n",
    "#45 is nonsensical, 47 is 85%\n",
    "rnd_seed = 45\n",
    "\n",
    "data_info = ['Breast_tissue', breast_data, feature_names, class_col, class_names, class_tags, rnd_seed]\n",
    "datasets.append(data_info)\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "data = breast_data\n",
    "print(data.head())\n",
    "print(feature_names)\n",
    "rf_gscv, top_features_lst, rare_class = fitRF_and_rank(data, feature_names, class_col, rnd_seed)\n",
    "for lab,feat in zip(['Global: ', 'Shap: ', 'Pcfi: ', 'Rare class Shap: ', 'Rare class Pcfi: '], top_features_lst):\n",
    "    print(lab,feat)\n",
    "label_lst = ['Global', 'Shap', 'Pcfi', 'Rare Shap', 'Rare Pfci']\n",
    "is_global_score = [True, True, True, False, False]\n",
    "scores = [refitRF(data, top_features, class_col, rf_gscv, rare_class, rnd_seed, global_score=global_score)\n",
    "             for global_score,top_features in zip(is_global_score,top_features_lst)]\n",
    "for lab,score in zip(label_lst, scores):\n",
    "    print(lab, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Buying' 'Maint' 'Doors' 'Persons' 'Lug_boot' 'Safety'] ['unacc' 'acc' 'good' 'vgood'] [0 1 2 3]\n",
      "   Buying  Maint  Doors  Persons  Lug_boot  Safety  Class\n",
      "0       3      3      2        0         0       0      0\n",
      "1       3      3      2        0         0       1      0\n",
      "2       3      3      2        0         0       2      0\n",
      "3       3      3      2        0         1       0      0\n",
      "4       3      3      2        0         1       1      0\n",
      "['Buying' 'Maint' 'Doors' 'Persons' 'Lug_boot' 'Safety']\n",
      "Is the dataset balanced? False\n",
      "[0 1 2 3] [1210  384   69   65]\n",
      "(array([0, 1, 2, 3]), array([908, 288,  51,  49]))\n",
      "(array([0, 1, 2, 3]), array([302,  96,  18,  16]))\n",
      "Best score: 0.8936141113688872\n",
      "Model performances:\n",
      "Accuracy: 97.22\n",
      "F1 macro score : 93.67\n",
      "Do y and y_train have same rare class? True\n",
      "[2, 3]\n",
      "0 2\n",
      "Global:  ['Safety' 'Persons' 'Maint']\n",
      "Shap:  ['Safety' 'Persons' 'Buying']\n",
      "Pcfi:  ['Safety' 'Persons' 'Maint']\n",
      "Rare class Shap:  ['Safety' 'Buying' 'Lug_boot']\n",
      "Rare class Pcfi:  ['Safety' 'Buying' 'Lug_boot']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "Global (76.16, 35.7)\n",
      "Shap (80.32, 50.56)\n",
      "Pcfi (76.16, 35.7)\n",
      "Rare Shap 0.4827586206896552\n",
      "Rare Pfci 0.4827586206896552\n"
     ]
    }
   ],
   "source": [
    "#Car data # SHAP GLOBAL\n",
    "\n",
    "col_names = np.array([\n",
    "    'buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class'\n",
    "])\n",
    "col_names = np.array([lab.capitalize() for lab in col_names])\n",
    "car_data = pd.read_csv(proj_path+'/0_data/car.data.csv', header=None, names=col_names)\n",
    "\n",
    "string_to_int_list = [\n",
    "    {'vhigh':3, 'high':2, 'med':1, 'low':0},\n",
    "    {'vhigh':3, 'high':2, 'med':1, 'low':0},\n",
    "    {'2':2, '3':3, '4':4, '5more':5},\n",
    "    {'2':0, '4':1, 'more':2},\n",
    "    {'small':0, 'med':1, 'big':2},\n",
    "    {'low':0, 'med':1, 'high':2},\n",
    "    {'unacc':0, 'acc':1, 'good':2, 'vgood':3}\n",
    "]\n",
    "\n",
    "for col_name,string_to_int in zip(car_data.columns, string_to_int_list):\n",
    "    car_data.loc[:,col_name] = car_data.apply(lambda r: string_to_int[r[col_name]], axis=1)\n",
    "\n",
    "    \n",
    "feature_names = np.array(car_data.columns[car_data.columns!='Class'])\n",
    "class_col = 'Class'\n",
    "class_names = np.array(['unacc', 'acc', 'good', 'vgood'])\n",
    "class_tags = np.arange(len(class_names))\n",
    "print(feature_names, class_names, class_tags)\n",
    "\n",
    "rnd_seed = 45\n",
    "data_info = ['Cars', car_data, feature_names, class_col, class_names, class_tags, rnd_seed]\n",
    "datasets.append(data_info)\n",
    "\n",
    "\n",
    "###\n",
    "data = car_data\n",
    "print(data.head())\n",
    "print(feature_names)\n",
    "rf_gscv, top_features_lst, rare_class = fitRF_and_rank(data, feature_names, class_col, rnd_seed)\n",
    "for lab,feat in zip(['Global: ', 'Shap: ', 'Pcfi: ', 'Rare class Shap: ', 'Rare class Pcfi: '], top_features_lst):\n",
    "    print(lab,feat)\n",
    "label_lst = ['Global', 'Shap', 'Pcfi', 'Rare Shap', 'Rare Pfci']\n",
    "is_global_score = [True, True, True, False, False]\n",
    "scores = [refitRF(data, top_features, class_col, rf_gscv, rare_class, rnd_seed, global_score=global_score)\n",
    "             for global_score,top_features in zip(is_global_score,top_features_lst)]\n",
    "for lab,score in zip(label_lst, scores):\n",
    "    print(lab, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Signal 1  Signal 2  Signal 3  Signal 4  Signal 5  Signal 6  Signal 7  Room\n",
      "0       -64       -56       -61       -66       -71       -82       -81     1\n",
      "1       -68       -57       -61       -65       -71       -85       -85     1\n",
      "2       -63       -60       -60       -67       -76       -85       -84     1\n",
      "3       -61       -60       -68       -62       -77       -90       -80     1\n",
      "4       -63       -65       -60       -63       -77       -81       -87     1\n",
      "['Signal 1' 'Signal 2' 'Signal 3' 'Signal 4' 'Signal 5' 'Signal 6'\n",
      " 'Signal 7']\n",
      "Is the dataset balanced? True\n",
      "[1 2 3 4] [500 500 500 500]\n",
      "(array([1, 2, 3, 4]), array([372, 379, 378, 371]))\n",
      "(array([1, 2, 3, 4]), array([128, 121, 122, 129]))\n",
      "Best score: 0.9847998472723013\n",
      "Model performances:\n",
      "Accuracy: 98.0\n",
      "F1 macro score : 97.99\n",
      "Do y and y_train have same rare class? False\n",
      "[]\n",
      "0 1\n",
      "0 2\n",
      "1 2\n",
      "Global:  ['Signal 1' 'Signal 5' 'Signal 4']\n",
      "Shap:  ['Signal 1' 'Signal 5' 'Signal 4']\n",
      "Pcfi:  ['Signal 1' 'Signal 5' 'Signal 4']\n",
      "Rare class Shap:  ['Signal 5' 'Signal 1' 'Signal 4']\n",
      "Rare class Pcfi:  ['Signal 5' 'Signal 1' 'Signal 4']\n",
      "1\n",
      "1\n",
      "Global (97.8, 97.79)\n",
      "Shap (97.8, 97.79)\n",
      "Pcfi (97.8, 97.79)\n",
      "Rare Shap 0.9808429118773946\n",
      "Rare Pfci 0.9808429118773946\n"
     ]
    }
   ],
   "source": [
    "#WiFi signals data #ALL =\n",
    "wifi_data = pd.read_excel(proj_path+'/0_data/WiFi_signals.xls')\n",
    "\n",
    "class_names = np.unique(wifi_data.Room)\n",
    "feature_names = np.array(wifi_data.columns[wifi_data.columns!='Room'])\n",
    "class_col = 'Room'\n",
    "class_tags = class_names\n",
    "#print(class_names, feature_names, np.unique(wifi_data.Room, return_counts=True))\n",
    "\n",
    "rnd_seed = 45\n",
    "\n",
    "data_info = ['Wifi', wifi_data, feature_names, class_col, class_names, class_tags, rnd_seed]\n",
    "datasets.append(data_info)\n",
    "\n",
    "\n",
    "###\n",
    "data = wifi_data\n",
    "print(data.head())\n",
    "print(feature_names)\n",
    "rf_gscv, top_features_lst, rare_class = fitRF_and_rank(data, feature_names, class_col, rnd_seed)\n",
    "for lab,feat in zip(['Global: ', 'Shap: ', 'Pcfi: ', 'Rare class Shap: ', 'Rare class Pcfi: '], top_features_lst):\n",
    "    print(lab,feat)\n",
    "label_lst = ['Global', 'Shap', 'Pcfi', 'Rare Shap', 'Rare Pfci']\n",
    "is_global_score = [True, True, True, False, False]\n",
    "scores = [refitRF(data, top_features, class_col, rf_gscv, rare_class, rnd_seed, global_score=global_score)\n",
    "             for global_score,top_features in zip(is_global_score,top_features_lst)]\n",
    "for lab,score in zip(label_lst, scores):\n",
    "    print(lab, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   Class  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "['sepal length (cm)' 'sepal width (cm)' 'petal length (cm)'\n",
      " 'petal width (cm)']\n",
      "Is the dataset balanced? True\n",
      "[0 1 2] [50 50 50]\n",
      "(array([0, 1, 2]), array([36, 41, 35]))\n",
      "(array([0, 1, 2]), array([14,  9, 15]))\n",
      "Best score: 0.9641562141562142\n",
      "Model performances:\n",
      "Accuracy: 94.74\n",
      "F1 macro score : 94.29\n",
      "Do y and y_train have same rare class? False\n",
      "[]\n",
      "0 1\n",
      "0 2\n",
      "1 2\n",
      "Global:  ['petal width (cm)' 'petal length (cm)' 'sepal length (cm)']\n",
      "Shap:  ['petal width (cm)' 'petal length (cm)' 'sepal length (cm)']\n",
      "Pcfi:  ['petal width (cm)' 'petal length (cm)' 'sepal length (cm)']\n",
      "Rare class Shap:  ['petal width (cm)' 'petal length (cm)' 'sepal length (cm)']\n",
      "Rare class Pcfi:  ['petal width (cm)' 'petal length (cm)' 'sepal length (cm)']\n",
      "0\n",
      "0\n",
      "Global (94.74, 94.29)\n",
      "Shap (94.74, 94.29)\n",
      "Pcfi (94.74, 94.29)\n",
      "Rare Shap 1.0\n",
      "Rare Pfci 1.0\n"
     ]
    }
   ],
   "source": [
    "#Import iris #ALL =\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "feature_names = np.array(iris.feature_names)\n",
    "class_names = iris.target_names\n",
    "class_tags = np.array([0, 1, 2])\n",
    "class_col = 'Class'\n",
    "\n",
    "iris_data = pd.DataFrame(iris.data,columns=feature_names)\n",
    "iris_data[class_col] = pd.Series(iris.target)\n",
    "\n",
    "rnd_seed = 45\n",
    "data_info = ['Iris', iris_data, feature_names, class_col, class_names, class_tags, rnd_seed]\n",
    "datasets.append(data_info)\n",
    "\n",
    "\n",
    "###\n",
    "data = iris_data\n",
    "print(data.head())\n",
    "print(feature_names)\n",
    "rf_gscv, top_features_lst, rare_class = fitRF_and_rank(data, feature_names, class_col, rnd_seed)\n",
    "for lab,feat in zip(['Global: ', 'Shap: ', 'Pcfi: ', 'Rare class Shap: ', 'Rare class Pcfi: '], top_features_lst):\n",
    "    print(lab,feat)\n",
    "label_lst = ['Global', 'Shap', 'Pcfi', 'Rare Shap', 'Rare Pfci']\n",
    "is_global_score = [True, True, True, False, False]\n",
    "scores = [refitRF(data, top_features, class_col, rf_gscv, rare_class, rnd_seed, global_score=global_score)\n",
    "             for global_score,top_features in zip(is_global_score,top_features_lst)]\n",
    "for lab,score in zip(label_lst, scores):\n",
    "    print(lab, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "\n",
      "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   od280/od315_of_diluted_wines  proline  Class  \n",
      "0                          3.92   1065.0      0  \n",
      "1                          3.40   1050.0      0  \n",
      "2                          3.17   1185.0      0  \n",
      "3                          3.45   1480.0      0  \n",
      "4                          2.93    735.0      0  \n",
      "['alcohol' 'malic_acid' 'ash' 'alcalinity_of_ash' 'magnesium'\n",
      " 'total_phenols' 'flavanoids' 'nonflavanoid_phenols' 'proanthocyanins'\n",
      " 'color_intensity' 'hue' 'od280/od315_of_diluted_wines' 'proline']\n",
      "Is the dataset balanced? False\n",
      "[0 1 2] [59 71 48]\n",
      "(array([0, 1, 2]), array([47, 54, 32]))\n",
      "(array([0, 1, 2]), array([12, 17, 16]))\n",
      "Best score: 0.9854528724093942\n",
      "Model performances:\n",
      "Accuracy: 100.0\n",
      "F1 macro score : 100.0\n",
      "Do y and y_train have same rare class? True\n",
      "[1, 2, 7, 8]\n",
      "0 1\n",
      "0 2\n",
      "1 2\n",
      "Global:  ['proline' 'flavanoids' 'color_intensity']\n",
      "Shap:  ['proline' 'flavanoids' 'color_intensity']\n",
      "Pcfi:  ['proline' 'flavanoids' 'color_intensity']\n",
      "Rare class Shap:  ['flavanoids' 'od280/od315_of_diluted_wines' 'color_intensity']\n",
      "Rare class Pcfi:  ['flavanoids' 'od280/od315_of_diluted_wines' 'color_intensity']\n",
      "2\n",
      "2\n",
      "Global (97.78, 97.97)\n",
      "Shap (97.78, 97.97)\n",
      "Pcfi (97.78, 97.97)\n",
      "Rare Shap 0.9696969696969697\n",
      "Rare Pfci 0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "#Import wine #ALL =\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wines = load_wine()\n",
    "feature_names = np.array(wines.feature_names)\n",
    "class_names = wines.target_names\n",
    "class_tags = np.array([0, 1, 2])\n",
    "class_col = 'Class'\n",
    "\n",
    "wine_data = pd.DataFrame(wines.data,columns=feature_names)\n",
    "wine_data[class_col] = pd.Series(wines.target)\n",
    "\n",
    "rnd_seed = 45\n",
    "data_info = ['Wine', wine_data, feature_names, class_col, class_names, class_tags, rnd_seed]\n",
    "datasets.append(data_info)\n",
    "\n",
    "\n",
    "###\n",
    "data = wine_data\n",
    "print(data.head())\n",
    "print(feature_names)\n",
    "rf_gscv, top_features_lst, rare_class = fitRF_and_rank(data, feature_names, class_col, rnd_seed)\n",
    "for lab,feat in zip(['Global: ', 'Shap: ', 'Pcfi: ', 'Rare class Shap: ', 'Rare class Pcfi: '], top_features_lst):\n",
    "    print(lab,feat)\n",
    "label_lst = ['Global', 'Shap', 'Pcfi', 'Rare Shap', 'Rare Pfci']\n",
    "is_global_score = [True, True, True, False, False]\n",
    "scores = [refitRF(data, top_features, class_col, rf_gscv, rare_class, rnd_seed, global_score=global_score)\n",
    "             for global_score,top_features in zip(is_global_score,top_features_lst)]\n",
    "for lab,score in zip(label_lst, scores):\n",
    "    print(lab, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
